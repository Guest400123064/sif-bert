{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Tuple, Callable, Union, Optional, Any\n",
    "import logging\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class BatchPCRemoval(nn.Module):\n",
    "    \n",
    "    def __init__(self, npc: int = 1):\n",
    "        super(BatchPCRemoval, self).__init__()\n",
    "        self.config_keys = [\"npc\"]\n",
    "        self.npc = npc\n",
    "\n",
    "    def forward(self, features: Dict[str, Any]):\n",
    "        \n",
    "        embeddings = features[\"sentence_embedding\"]\n",
    "        embeddings_detached = embeddings.detach()\n",
    "\n",
    "        _, _, pcs = torch.linalg.svd(embeddings_detached, full_matrices=False)\n",
    "        pcs = pcs[:self.npc]\n",
    "        proj = torch.matmul(embeddings_detached, pcs.transpose(1, 0))\n",
    "\n",
    "        features.update({\"sentence_embedding\": embeddings - torch.matmul(proj, pcs)})\n",
    "        return features\n",
    "        \n",
    "    # For IO //////////////////////////////////////////////////////////////////////\n",
    "    def get_config_dict(self):\n",
    "        return {key: self.__dict__[key] for key in self.config_keys}\n",
    "\n",
    "    def save(self, output_path):\n",
    "        with open(os.path.join(output_path, \"config.json\"), \"w\") as f:\n",
    "            json.dump(self.get_config_dict(), f, indent=2)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, input_path):\n",
    "        with open(os.path.join(input_path, \"config.json\")) as f:\n",
    "            config = json.load(f)\n",
    "        return cls(**config)\n",
    "\n",
    "\n",
    "class DirectionRemover(nn.Module):\n",
    "    \n",
    "    def __init__(self, direction: List[float]):\n",
    "        super(DirectionRemover, self).__init__()\n",
    "        self.direction = nn.Parameter(torch.FloatTensor(direction), requires_grad=False)\n",
    "        \n",
    "    def forward(self, features: Dict[str, torch.Tensor]):\n",
    "        \"\"\"Performs direction removal by first project the sentence embedding onto the\n",
    "            direction specified by `self.direction` and then subtracting the projection.`\"\"\"\n",
    "        \n",
    "        sentence_embedding = features[\"sentence_embedding\"]\n",
    "        sentence_embedding = sentence_embedding - torch.sum(sentence_embedding * self.direction, dim=1, keepdim=True) * self.direction\n",
    "        features.update({\"sentence_embedding\": sentence_embedding})\n",
    "        return features\n",
    "\n",
    "    \n",
    "def compute_pc(X, npc=1):\n",
    "    \"\"\"Compute the principal components. DO NOT MAKE THE DATA ZERO MEAN!\n",
    "    :param X: \n",
    "        The sentence embedding matrix; `X[i,:]` is a data point\n",
    "    :param npc: \n",
    "        Number of principal components to remove\n",
    "    :return: \n",
    "        `component_[i,:]` is the i-th pc\n",
    "    \"\"\"\n",
    "\n",
    "    svd = TruncatedSVD(n_components=npc, n_iter=7, random_state=0)\n",
    "    svd.fit(X)\n",
    "    return svd.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration mteb--stsbenchmark-sts-998a21523b45a16a\n",
      "Found cached dataset json (/home/dogdog/.cache/huggingface/datasets/mteb___json/mteb--stsbenchmark-sts-998a21523b45a16a/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "861116ab3a5b4286b2320527b97af884",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset sick (/home/dogdog/.cache/huggingface/datasets/sick/default/0.0.0/c6b3b0b44eb84b134851396d6d464e5cb8f026960519d640e087fe33472626db)\n",
      "Found cached dataset wikitext (/home/dogdog/.cache/huggingface/datasets/wikitext/wikitext-2-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "from sentence_transformers.models import Transformer, Pooling\n",
    "\n",
    "from src.models import SIFPooling\n",
    "\n",
    "from datasets import load_dataset, get_dataset_config_names\n",
    "\n",
    "\n",
    "model_card = \"bert-base-cased\"\n",
    "\n",
    "data_stsb = load_dataset(\"mteb/stsbenchmark-sts\")\n",
    "eval_stsb = EmbeddingSimilarityEvaluator(sentences1=data_stsb[\"test\"][\"sentence1\"], \n",
    "                                         sentences2=data_stsb[\"test\"][\"sentence2\"], \n",
    "                                         scores=data_stsb[\"test\"][\"score\"], \n",
    "                                         name=\"stsb-dev\",\n",
    "                                         write_csv=False,\n",
    "                                         batch_size=512)\n",
    "\n",
    "data_sick = load_dataset(\"sick\", split=\"test\")\n",
    "eval_sick = EmbeddingSimilarityEvaluator(sentences1=data_sick[\"sentence_A\"],\n",
    "                                         sentences2=data_sick[\"sentence_B\"],\n",
    "                                         scores=data_sick[\"relatedness_score\"],\n",
    "                                         name=\"sick-dev\",\n",
    "                                         write_csv=False,\n",
    "                                         batch_size=512)\n",
    "\n",
    "# Load wiki-text-2 to estimate the word frequencies\n",
    "corpus = [s.strip() for s in load_dataset(\"wikitext\", \"wikitext-2-v1\", split=\"train\")[\"text\"]\n",
    "            if s.strip() != \"\" and not s.strip().startswith(\"=\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (726 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = Transformer(model_card)\n",
    "normal_pooling_layer = Pooling(embedding_layer.get_word_embedding_dimension(), pooling_mode=\"mean\")\n",
    "weighted_pooling_layer = SIFPooling.from_corpus_hf(model_card, corpus)\n",
    "batch_pc_removal_layer = BatchPCRemoval(npc=1)\n",
    "\n",
    "model_b = SentenceTransformer(modules=[embedding_layer, normal_pooling_layer])\n",
    "model_w = SentenceTransformer(modules=[embedding_layer, weighted_pooling_layer])\n",
    "model_r = SentenceTransformer(modules=[embedding_layer, weighted_pooling_layer, batch_pc_removal_layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5910148950535766"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_sick(model_b, output_path=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6030681625701952"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_sick(model_w, output_path=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6409534386135679"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_sick(model_r, output_path=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conceptor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "458244594d2cffc8461b9ca601e91cdf31df118d80684fec69912c05d67471fc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
