{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Tuple, Callable, Union, Optional, Any\n",
    "import logging\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class DirectionRemover(nn.Module):\n",
    "    \n",
    "    def __init__(self, direction: List[float]):\n",
    "        super(DirectionRemover, self).__init__()\n",
    "        self.direction = nn.Parameter(torch.FloatTensor(direction), requires_grad=False)\n",
    "        \n",
    "    def forward(self, features: Dict[str, torch.Tensor]):\n",
    "        \"\"\"Performs direction removal by first project the sentence embedding onto the\n",
    "            direction specified by `self.direction` and then subtracting the projection.`\"\"\"\n",
    "        \n",
    "        sentence_embedding = features[\"sentence_embedding\"]\n",
    "        sentence_embedding = sentence_embedding - torch.sum(sentence_embedding * self.direction, dim=1, keepdim=True) * self.direction\n",
    "        features.update({\"sentence_embedding\": sentence_embedding})\n",
    "        return features\n",
    "\n",
    "    \n",
    "def compute_pc(X, npc=1):\n",
    "    \"\"\"Compute the principal components. DO NOT MAKE THE DATA ZERO MEAN!\n",
    "    :param X: \n",
    "        The sentence embedding matrix; `X[i,:]` is a data point\n",
    "    :param npc: \n",
    "        Number of principal components to remove\n",
    "    :return: \n",
    "        `component_[i,:]` is the i-th pc\n",
    "    \"\"\"\n",
    "\n",
    "    svd = TruncatedSVD(n_components=npc, n_iter=7, random_state=0)\n",
    "    svd.fit(X)\n",
    "    return svd.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration mteb--stsbenchmark-sts-998a21523b45a16a\n",
      "Found cached dataset json (/home/dogdog/.cache/huggingface/datasets/mteb___json/mteb--stsbenchmark-sts-998a21523b45a16a/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af7afa5b55d54e5eb4ca5a756a97fa23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset sick (/home/dogdog/.cache/huggingface/datasets/sick/default/0.0.0/c6b3b0b44eb84b134851396d6d464e5cb8f026960519d640e087fe33472626db)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "from sentence_transformers.models import Transformer, Pooling\n",
    "\n",
    "from src.models import SIFPooling\n",
    "\n",
    "from datasets import load_dataset, get_dataset_config_names\n",
    "\n",
    "\n",
    "model_card = \"bert-base-uncased\"\n",
    "\n",
    "data_stsb = load_dataset(\"mteb/stsbenchmark-sts\")\n",
    "eval_stsb = EmbeddingSimilarityEvaluator(sentences1=data_stsb[\"test\"][\"sentence1\"], \n",
    "                                         sentences2=data_stsb[\"test\"][\"sentence2\"], \n",
    "                                         scores=data_stsb[\"test\"][\"score\"], \n",
    "                                         name=\"stsb-dev\",\n",
    "                                         write_csv=False)\n",
    "\n",
    "data_sick = load_dataset(\"sick\", split=\"test\")\n",
    "eval_sick = EmbeddingSimilarityEvaluator(sentences1=data_sick[\"sentence_A\"],\n",
    "                                         sentences2=data_sick[\"sentence_B\"],\n",
    "                                         scores=data_sick[\"relatedness_score\"],\n",
    "                                         name=\"sick-dev\",\n",
    "                                         write_csv=False)\n",
    "\n",
    "corpus = [s.encode(\"utf-8\").decode(\"utf-8\") for s in \n",
    "            data_stsb[\"train\"][\"sentence1\"] + data_stsb[\"train\"][\"sentence2\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = Transformer(model_card)\n",
    "normal_pooling_layer = Pooling(embedding_layer.get_word_embedding_dimension(), pooling_mode=\"mean\")\n",
    "weighted_pooling_layer = SIFPooling.from_corpus_hf(model_card, corpus)\n",
    "\n",
    "model_b = SentenceTransformer(modules=[embedding_layer, normal_pooling_layer])\n",
    "model_w = SentenceTransformer(modules=[embedding_layer, weighted_pooling_layer])\n",
    "\n",
    "# normal_dr_layer = DirectionRemover(compute_pc(model_b.encode(corpus)))\n",
    "# weighted_dr_layer = DirectionRemover(compute_pc(model_w.encode(corpus)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w.save(\"../models/sif-bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): SIFPooling(\n",
       "    (emb_layer): Embedding(30522, 1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_w.load(\"../models/sif-bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48862744348863496"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_stsb(model_b, output_path=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6453034678942327"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_stsb(model_w, output_path=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5503251654706054"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_br = SentenceTransformer(modules=[embedding_layer, normal_pooling_layer, normal_dr_layer])\n",
    "eval_stsb(model_br, output_path=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6581081183448381"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_wr = SentenceTransformer(modules=[embedding_layer, weighted_pooling_layer, weighted_dr_layer])\n",
    "eval_stsb(model_wr, output_path=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = models.Transformer(model_card)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_layer_pooling = models.WeightedLayerPooling(embedding_layer.get_word_embedding_dimension(),\n",
    "                                                  num_hidden_layers=12,\n",
    "                                                  layer_start=11)\n",
    "\n",
    "model_t = SentenceTransformer(modules=[embedding_layer, cross_layer_pooling, weighted_pooling_layer, weighted_dr_layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6619439511647357"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_stsb(model_t, output_path=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conceptor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "458244594d2cffc8461b9ca601e91cdf31df118d80684fec69912c05d67471fc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
